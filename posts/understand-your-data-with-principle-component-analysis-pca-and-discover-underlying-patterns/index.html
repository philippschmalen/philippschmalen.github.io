<!DOCTYPE html>
<html lang="en-us">
  <head>
    <title>Understand your data with principle component analysis (PCA) and discover underlying patterns | Philipp Schmalen</title>

    <meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">    
<meta name="viewport" content="width=device-width,minimum-scale=1">
<meta name="description" content="with windows subsystem for Linux (WSL)">
<meta name="generator" content="Hugo 0.80.0" />


  <META NAME="ROBOTS" CONTENT="NOINDEX, NOFOLLOW">


<link rel="stylesheet" href="/css/style.css">
<link rel="shortcut icon" href="/images/favicon.ico" type="image/x-icon" />

 
    
<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
	ga('create', 'UA-154596617-1', 'auto');
	
	ga('send', 'pageview');
}
</script>
<script async src='https://www.google-analytics.com/analytics.js'></script>








  </head>

  <body>
    <nav class="navigation">
	
		<a href="/"> <span class="arrow">←</span>Home</a>
	
	
	

	

	<a href="/about">About</a>

	

	
</nav>


    <main class="main">
      

<section id="single">
    <h1 class="title">Understand your data with principle component analysis (PCA) and discover underlying patterns</h1>

    <div class="tip">
        <span>
          Aug 15, 2020
        </span>
        <span class="split">
          ·
        </span>

        <span>
          15 minute read
        </span>
    </div>

    <div class="content">
      <h1 id="understand-your-data-with-principle-component-analysis-pca-and-discover-underlying-patterns---enhanced-data-exploration-that-goes-beyond-descriptives">Understand your data with principle component analysis (PCA) and discover underlying patterns - Enhanced data exploration that goes beyond descriptives 

</h1><p>Save time, resources and stay healthy with data exploration that goes beyond means, distributions and correlations: Leverage PCA to see through the surface of variables. It saves time and resources, because it uncovers data issues <em>before</em> an hour-long model training and is good for a programmer&rsquo;s health, since she trades off data worries with something more enjoyable. For example, a well-proven machine learning model might fail, because of one-dimensional data with insufficient variance or other related issues. PCA offers valuable insights that make you confident about data properties and its hidden dimensions. </p>
<p>This article shows how to leverage PCA to understand key properties of a dataset, saving time and resources down the road which ultimately leads to a happier, more fulfilled coding life. I hope this post helps to apply PCA in a consistent way and understand its results.</p>
<p><p class="markdown-image">
  <img src="/figures/pca_exploration/gymnastics_yoga.jpg" alt="Photo by Burst on Pexels"  />
</p></p>
<blockquote>
<p>&ldquo;Beauty and health require not only education and knowledge, but also thorough data exploration.&rdquo; (~Plato)</p>
</blockquote>
<h2 id="tldr">TL;DR 

</h2><p>PCA provides valuable insights that reach beyond descriptive statistics and help to discover underlying patterns. Two PCA metrics indicate 1. how many components capture the largest share of variance (explained variance), and 2., which features correlate with the most important components (factor loading). <strong>These metrics crosscheck previous steps in the project work flow</strong>, such as data collection which then can be adjusted . As a shortcut and ready-to-use tool, I provide the function <code>do_pca()</code> which conducts a PCA for a prepared dataset to inspect its results within seconds in <a 
    href="https://github.com/philippschmalen/Code-snippets/blob/master/PCA_workflow.ipynb"
    
    
     
      target="_blank" 
      rel="noopener"
    
>
    this notebook
</a> or <a 
    href="https://github.com/philippschmalen/Code-snippets/blob/master/helper_pca.py"
    
    
     
      target="_blank" 
      rel="noopener"
    
>
    this script
</a>.</p>
<h2 id="data-exploration-as-a-safety-net">Data exploration as a safety net 

</h2><p>When a project structure resembles the one below, the prepared dataset is under scrutiny in the 4. step by looking at descriptive statistics. Among the most common ones are means, distributions and correlations taken across all observations or certain subgroups.</p>
<p><strong>Common project structure</strong></p>
<table>
<thead>
<tr>
<th>Project Step</th>
<th></th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>Collection</td>
<td>gather, retrieve or load data</td>
</tr>
<tr>
<td>2</td>
<td>Processing</td>
<td>Format raw data, handle missings</td>
</tr>
<tr>
<td>3</td>
<td>Engineering</td>
<td>Construct and select features</td>
</tr>
<tr>
<td><strong>4</strong></td>
<td><strong>Exploration</strong></td>
<td><strong>Inspect descriptives, decompositions</strong></td>
</tr>
<tr>
<td>5</td>
<td>Modelling</td>
<td>Train, validate and test models</td>
</tr>
<tr>
<td>6</td>
<td>Evaluation</td>
<td>Inspect results, compare models</td>
</tr>
</tbody>
</table>
<p>When the moment arrives of having a clean dataset after hours of work, makes many glance already towards the exciting step of applying models to the data. At this stage, possibly 80% of the project&rsquo;s workload is done, if the data did not fell out of the sky, cleaned and processed. Of course, the urge is strong for modeling, but here are two reasons why a thorough data exploration saves time down the road:</p>
<ol>
<li>catch coding errors $\rightarrow$ revise feature engineering (step 3)</li>
<li>identify underlying properties $\rightarrow$ rethink data collection (step 1), preprocessing (step 2) or feature engineering (step 3)</li>
</ol>
<p>Wondering about underperforming models due to underlying data issues after a few hours into training, validating and testing is like a photographer on the set, not knowing how their models might look like. Therefore, the key message is to see data exploration as an opportunity to get to know your data, understanding its strength and weaknesses.</p>
<p>Descriptive statistics often reveal coding errors. However, detecting underlying issues likely requires more than that. Decomposition methods such as PCA help to identify these and enable to revise previous steps. This ensures a smooth transition to model building.</p>
<p><p class="markdown-image">
  <img src="/figures/pca_exploration/photographer_set.jpg" alt="Photographer on set"  />
</p></p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<h2 id="look-beneath-the-surface-with-pca">Look beneath the surface with PCA 

</h2><p>Large datasets often require PCA to reduce dimensionality anyway. The method as such captures the maximum possible variance across features and projects observations onto mutually uncorrelated vectors, called components. Still, PCA serves other purposes than dimensionality reduction. It also helps to discover underlying patterns across features.</p>
<p>To focus on the implementation in Python instead of methodology, I will skip describing PCA in its workings. There exist many great resources about it that I refer to those instead:</p>
<ul>
<li>Animations showing PCA in action: <a href="https://setosa.io/ev/principal-component-analysis/">https://setosa.io/ev/principal-component-analysis/</a></li>
<li>PCA explained in a family conversation: <a href="https://stats.stackexchange.com/a/140579">https://stats.stackexchange.com/a/140579</a></li>
<li>Smith (2002). A tutorial on principal components analysis: <a 
    href="https://ourarchive.otago.ac.nz/bitstream/handle/10523/7534/OUCS-2002-12.pdf"
    
    
     
      target="_blank" 
      rel="noopener"
    
>
    Accessible here
</a>.</li>
</ul>
<p>Two metrics are crucial to make sense of PCA:</p>
<blockquote>
<p><strong>Explained variance</strong> measures how much a model can reflect the variance of the whole data. Principle components try to capture as much of the variance as possible and this measure shows to what extent they can do that. It helps to see  Components are sorted by explained variance, with the first one scoring highest and with a total sum of up to 1 across all components.</p>
</blockquote>
<blockquote>
<p><strong>Factor loading</strong> indicates how much a variable correlates with a component. Each component is made of a linear combination of variables, where some might have more weight than others. Factor loadings indicate this as correlation coefficients, ranging from -1 to 1, and make components interpretable.</p>
</blockquote>
<p>The upcoming sections apply PCA to exciting data from a behavioral field experiment and guide through using these metrics to enhance data exploration.</p>
<h2 id="load-data-a-randomized-educational-intervention-on-grit-alan-et-al-2019">Load data: A Randomized Educational Intervention on Grit (Alan et al. 2019) 

</h2><p>The iris dataset served well as a canonical example of several PCA. In an effort to be diverse and using novel data from a field study, I rely on replication data from Alan et al. [1]. I hope this is appreciated.</p>
<p>It comprises data from behavioral experiments at Turkish schools, where 10 year olds took part in a curriculum to improve a non-cognitive skill called grit which defines as perseverance to pursue a task. The authors sampled individual characteristics and conducted behavioral experiments to measure a potential treatment effect between those receiving the program (<code>grit == 1</code>) and those taking part in a control treatment (<code>grit == 0</code>).</p>
<p>(Interested in causal inference using a modified random forest? Check out my preliminary blog post on <a href="https://philippschmalen.github.io/posts/working_on_causal_forest/">https://philippschmalen.github.io/posts/working_on_causal_forest/</a> ).</p>
<p>I selected features according to their replication scripts, accessible on <a 
    href="https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/SAVGAL"
    
    
     
      target="_blank" 
      rel="noopener"
    
>
    Harvard Dataverse
</a> and solely used sample 2 (&ldquo;sample B&rdquo; in the publicly accessible <a 
    href="https://www.povertyactionlab.org/sites/default/files/research-paper/5555_Ever-Falied-Try-Again--Succeed-Better_Alan-Ertac-Boneva_March2016.pdf"
    
    
     
      target="_blank" 
      rel="noopener"
    
>
    working paper
</a>). To be concise, refer to the paper for relevant descriptives.</p>
<p>The following loads the data from an URL and stores it as a pandas dataframe.</p>
<div class="highlight"><pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">
<span style="color:#080;font-style:italic"># To load data from Harvard Dataverse</span>
<span style="color:#a2f;font-weight:bold">import</span> <span style="color:#00f;font-weight:bold">io</span> 
<span style="color:#a2f;font-weight:bold">import</span> <span style="color:#00f;font-weight:bold">requests</span> 

<span style="color:#080;font-style:italic"># load exciting data from URL (at least something else than Iris)</span>
url <span style="color:#666">=</span> <span style="color:#b44">&#39;https://dataverse.harvard.edu/api/access/datafile/3352340?gbrecs=false&#39;</span>
s <span style="color:#666">=</span> requests<span style="color:#666">.</span>get(url)<span style="color:#666">.</span>content

<span style="color:#080;font-style:italic"># store as dataframe</span>
df_raw <span style="color:#666">=</span> pd<span style="color:#666">.</span>read_csv(io<span style="color:#666">.</span>StringIO(s<span style="color:#666">.</span>decode(<span style="color:#b44">&#39;utf-8&#39;</span>)), sep<span style="color:#666">=</span><span style="color:#b44">&#39;</span><span style="color:#b62;font-weight:bold">\t</span><span style="color:#b44">&#39;</span>)
</code></pre></div><p><p class="markdown-image">
  <img src="/figures/pca_exploration/child_drawing_square.jpg" alt="Child draws PCA"  />
</p></p>
<h2 id="preprocessing-and-feature-engineering">Preprocessing and feature engineering 

</h2><p>For PCA to work, the data needs to be numeric, without missings, and standardized. I put all steps into one function (<code>clean_data()</code>) which returns a dataframe with standardized features. and conduct steps 1 to 3 of the project work flow (collecting, processing and engineering). 
To begin with, import necessary modules and packages.</p>
<div class="highlight"><pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#a2f;font-weight:bold">import</span> <span style="color:#00f;font-weight:bold">pandas</span> <span style="color:#a2f;font-weight:bold">as</span> <span style="color:#00f;font-weight:bold">pd</span>
<span style="color:#a2f;font-weight:bold">import</span> <span style="color:#00f;font-weight:bold">numpy</span> <span style="color:#a2f;font-weight:bold">as</span> <span style="color:#00f;font-weight:bold">np</span>

<span style="color:#080;font-style:italic"># sklearn module</span>
<span style="color:#a2f;font-weight:bold">from</span> <span style="color:#00f;font-weight:bold">sklearn.decomposition</span> <span style="color:#a2f;font-weight:bold">import</span> PCA

<span style="color:#080;font-style:italic"># plots</span>
<span style="color:#a2f;font-weight:bold">import</span> <span style="color:#00f;font-weight:bold">matplotlib.pyplot</span> <span style="color:#a2f;font-weight:bold">as</span> <span style="color:#00f;font-weight:bold">plt</span>
<span style="color:#a2f;font-weight:bold">import</span> <span style="color:#00f;font-weight:bold">seaborn</span> <span style="color:#a2f;font-weight:bold">as</span> <span style="color:#00f;font-weight:bold">sns</span>
<span style="color:#080;font-style:italic"># seaborn settings</span>
sns<span style="color:#666">.</span>set_style(<span style="color:#b44">&#34;whitegrid&#34;</span>)
sns<span style="color:#666">.</span>set_context(<span style="color:#b44">&#34;talk&#34;</span>)

<span style="color:#080;font-style:italic"># imports for function</span>
<span style="color:#a2f;font-weight:bold">from</span> <span style="color:#00f;font-weight:bold">sklearn.preprocessing</span> <span style="color:#a2f;font-weight:bold">import</span> StandardScaler
<span style="color:#a2f;font-weight:bold">from</span> <span style="color:#00f;font-weight:bold">sklearn.impute</span> <span style="color:#a2f;font-weight:bold">import</span> SimpleImputer
</code></pre></div><p>Next, the <code>clean_data</code> function is defined. It gives a shortcut to transform the raw data into a prepared dataset with (i.) selected features, (ii.) missings replaced by column means, and (iii.) standardized variables. Preparing the data takes one line of code, (iv).</p>
<div class="highlight"><pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#a2f;font-weight:bold">def</span> <span style="color:#00a000">clean_data</span>(data, select_X<span style="color:#666">=</span>None, impute<span style="color:#666">=</span>False, std<span style="color:#666">=</span>False): 
    <span style="color:#b44">&#34;&#34;&#34;Returns dataframe with selected, imputed and standardized features
</span><span style="color:#b44">    
</span><span style="color:#b44">    Input
</span><span style="color:#b44">          data: dataframe
</span><span style="color:#b44">          select_X: list of feature names to be selected (string)
</span><span style="color:#b44">          impute: If True impute np.nan with mean
</span><span style="color:#b44">          std: If True standardize data
</span><span style="color:#b44">          
</span><span style="color:#b44">    Return
</span><span style="color:#b44">        dataframe: data with selected, imputed and standardized features    
</span><span style="color:#b44">    &#34;&#34;&#34;</span>
    
    <span style="color:#080;font-style:italic"># (i.) select features</span>
    <span style="color:#a2f;font-weight:bold">if</span> select_X <span style="color:#a2f;font-weight:bold">is</span> <span style="color:#a2f;font-weight:bold">not</span> None:
        data <span style="color:#666">=</span> data<span style="color:#666">.</span>filter(select_X, axis<span style="color:#666">=</span><span style="color:#b44">&#39;columns&#39;</span>)
        <span style="color:#a2f;font-weight:bold">print</span>(<span style="color:#b44">&#34;</span><span style="color:#b62;font-weight:bold">\t</span><span style="color:#b44">&gt;&gt;&gt; Selected features: {}&#34;</span><span style="color:#666">.</span>format(select_X))
    <span style="color:#a2f;font-weight:bold">else</span>:
        <span style="color:#080;font-style:italic"># store column names</span>
        select_X <span style="color:#666">=</span> <span style="color:#a2f">list</span>(data<span style="color:#666">.</span>columns)
    
    <span style="color:#080;font-style:italic"># (ii.) impute with mean </span>
    <span style="color:#a2f;font-weight:bold">if</span> impute:
        imp <span style="color:#666">=</span> SimpleImputer()
        data <span style="color:#666">=</span> imp<span style="color:#666">.</span>fit_transform(data)
        <span style="color:#a2f;font-weight:bold">print</span>(<span style="color:#b44">&#34;</span><span style="color:#b62;font-weight:bold">\t</span><span style="color:#b44">&gt;&gt;&gt; Imputed missings&#34;</span>)
    
    <span style="color:#080;font-style:italic"># (iii.) standardize </span>
    <span style="color:#a2f;font-weight:bold">if</span> std:
        std_scaler <span style="color:#666">=</span> StandardScaler()
        data <span style="color:#666">=</span> std_scaler<span style="color:#666">.</span>fit_transform(data)
        <span style="color:#a2f;font-weight:bold">print</span>(<span style="color:#b44">&#34;</span><span style="color:#b62;font-weight:bold">\t</span><span style="color:#b44">&gt;&gt;&gt; Standardized data&#34;</span>)
    
    <span style="color:#a2f;font-weight:bold">return</span> pd<span style="color:#666">.</span>DataFrame(data, columns<span style="color:#666">=</span>select_X)


<span style="color:#080;font-style:italic"># select relevant features in line with Alan et al. (2019)</span>
selected_features <span style="color:#666">=</span> [<span style="color:#b44">&#39;grit&#39;</span>, <span style="color:#b44">&#39;male&#39;</span>, <span style="color:#b44">&#39;task_ability&#39;</span>, <span style="color:#b44">&#39;raven&#39;</span>, <span style="color:#b44">&#39;grit_survey1&#39;</span> 
 <span style="color:#b44">&#39;belief_survey1&#39;</span>, <span style="color:#b44">&#39;mathscore1&#39;</span>, <span style="color:#b44">&#39;verbalscore1&#39;</span>, <span style="color:#b44">&#39;risk&#39;</span>, <span style="color:#b44">&#39;inconsistent&#39;</span>]


<span style="color:#080;font-style:italic"># (iv.) select features, impute missings and standardize</span>
X_std <span style="color:#666">=</span> clean_data(df_raw, selected_features, impute<span style="color:#666">=</span>True, std<span style="color:#666">=</span>True)
</code></pre></div><p>Now, the data is ready for exploration.</p>
<h2 id="scree-plots-and-factor-loadings-interpret-pca-results">Scree plots and factor loadings: Interpret PCA results 

</h2><p>A PCA yields two metrics that are relevant for data exploration: Firstly, how much variance each component explains (scree plot), and secondly how much a variable correlates with a component (factor loading). The following sections provide a practical example and guide through the PCA output with a scree plot for explained variance and a heatmap on factor loadings.</p>
<h3 id="explained-variance-shows-the-number-of-dimensions-across-variables">Explained variance shows the number of dimensions across variables 

</h3><p>Nowadays, data is abundant and the size of datasets continues to grow. Data scientists routinely deal with hundreds of variables. However, are these variables worth their memory? Put differently: Does a variable capture unique patterns or does it measure similar properties already reflected by other variables?</p>
<p>PCA might answer this through the metric of explained variance per component. It details the number of underlying dimensions on which most of the variance is observed.</p>
<p>The code below initializes a PCA object from sklearn and transforms the original data along the calculated components (i.).  Thereafter, information on explained variance is retrieved (ii.) and printed (iii.).</p>
<div class="highlight"><pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#080;font-style:italic"># (i.) initialize and compute pca</span>
pca <span style="color:#666">=</span> PCA()
X_pca <span style="color:#666">=</span> pca<span style="color:#666">.</span>fit_transform(X_std)

<span style="color:#080;font-style:italic"># (ii.) get basic info</span>
n_components <span style="color:#666">=</span> <span style="color:#a2f">len</span>(pca<span style="color:#666">.</span>explained_variance_ratio_)
explained_variance <span style="color:#666">=</span> pca<span style="color:#666">.</span>explained_variance_ratio_
cum_explained_variance <span style="color:#666">=</span> np<span style="color:#666">.</span>cumsum(explained_variance)
idx <span style="color:#666">=</span> np<span style="color:#666">.</span>arange(n_components)<span style="color:#666">+</span><span style="color:#666">1</span>

df_explained_variance <span style="color:#666">=</span> pd<span style="color:#666">.</span>DataFrame([explained_variance, cum_explained_variance], 
                                     index<span style="color:#666">=</span>[<span style="color:#b44">&#39;explained variance&#39;</span>, <span style="color:#b44">&#39;cumulative&#39;</span>], 
                                     columns<span style="color:#666">=</span>idx)<span style="color:#666">.</span>T

mean_explained_variance <span style="color:#666">=</span> df_explained_variance<span style="color:#666">.</span>iloc[:,<span style="color:#666">0</span>]<span style="color:#666">.</span>mean() <span style="color:#080;font-style:italic"># calculate mean explained variance</span>

<span style="color:#080;font-style:italic"># (iii.) Print explained variance as plain text</span>
<span style="color:#a2f;font-weight:bold">print</span>(<span style="color:#b44">&#39;PCA Overview&#39;</span>)
<span style="color:#a2f;font-weight:bold">print</span>(<span style="color:#b44">&#39;=&#39;</span><span style="color:#666">*</span><span style="color:#666">40</span>)
<span style="color:#a2f;font-weight:bold">print</span>(<span style="color:#b44">&#34;Total: {} components&#34;</span><span style="color:#666">.</span>format(n_components))
<span style="color:#a2f;font-weight:bold">print</span>(<span style="color:#b44">&#39;-&#39;</span><span style="color:#666">*</span><span style="color:#666">40</span>)
<span style="color:#a2f;font-weight:bold">print</span>(<span style="color:#b44">&#39;Mean explained variance:&#39;</span>, <span style="color:#a2f">round</span>(mean_explained_variance,<span style="color:#666">3</span>))
<span style="color:#a2f;font-weight:bold">print</span>(<span style="color:#b44">&#39;-&#39;</span><span style="color:#666">*</span><span style="color:#666">40</span>)
<span style="color:#a2f;font-weight:bold">print</span>(df_explained_variance<span style="color:#666">.</span>head(<span style="color:#666">20</span>))
<span style="color:#a2f;font-weight:bold">print</span>(<span style="color:#b44">&#39;-&#39;</span><span style="color:#666">*</span><span style="color:#666">40</span>)
</code></pre></div><div class="highlight"><pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-fallback" data-lang="fallback">PCA Overview
========================================
Total: 10 components
----------------------------------------
Mean explained variance: 0.1
----------------------------------------
    explained variance  cumulative
1             0.265261    0.265261
2             0.122700    0.387962
3             0.113990    0.501951
4             0.099139    0.601090
5             0.094357    0.695447
6             0.083412    0.778859
7             0.063117    0.841976
8             0.056386    0.898362
9             0.052588    0.950950
10            0.049050    1.000000
----------------------------------------
</code></pre></div><p><strong>Interpretation:</strong> The first component makes up for around 27% of the explained variance. This is relatively low as compared to other datasets, but no matter of concern. It simply indicates that a major share (100%-27%=73%) of observations distributes across more than one dimension.</p>
<p>Another way to approach the output is to ask: How much components are required to cover more than X% of the variance? For example, I want to reduce the data&rsquo;s dimensionality retain at least 90% variance of the original data. Then I would have to include 9 components to reach at least 90% and even have 95% of explained variance covered in this case. With an overall of 10 variables in the original dataset, the scope to reduce dimensionality is limited. Additionally, this shows that each of the 10 original variables adds somewhat unique patterns and limitedly repeats information from other variables.</p>
<p><p class="markdown-image">
  <img src="/figures/pca_exploration/wine_cellar.jpg" alt="Wine cellar"  />
</p></p>
<p>To give another example, I list explained variance of &ldquo;the&rdquo; <a 
    href="https://archive.ics.uci.edu/ml/datasets/wine"
    
    
     
      target="_blank" 
      rel="noopener"
    
>
    wine dataset
</a>:</p>
<div class="highlight"><pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-fallback" data-lang="fallback">PCA Overview: Wine dataset
========================================
Total: 13 components
----------------------------------------
Mean explained variance: 0.077
----------------------------------------
    explained variance  cumulative
1             0.361988    0.361988
2             0.192075    0.554063
3             0.111236    0.665300
4             0.070690    0.735990
5             0.065633    0.801623
6             0.049358    0.850981
7             0.042387    0.893368
8             0.026807    0.920175
9             0.022222    0.942397
10            0.019300    0.961697
11            0.017368    0.979066
12            0.012982    0.992048
13            0.007952    1.000000
----------------------------------------
</code></pre></div><p>Here, 8 out of 13 components suffice to capture at least 90% of the original variance. Thus, there is more scope to reduce dimensionality. Furthermore it indicates that some variables do not contribute much to variance in the data.</p>
<p>Instead of plain text, a <strong>scree plot</strong> visualizes explained variance across components and informs about individual and cumulative explained variance for each component. The next code chunk creates such a scree plot and includes an option to focus on the first X components to be manageable when dealing with hundreds of components for larger datasets (<code>limit</code>).</p>
<div class="highlight"><pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#080;font-style:italic">#limit plot to x PC</span>
limit <span style="color:#666">=</span> <span style="color:#a2f">int</span>(<span style="color:#a2f">input</span>(<span style="color:#b44">&#34;Limit scree plot to nth component (0 for all) &gt; &#34;</span>))
<span style="color:#a2f;font-weight:bold">if</span> limit <span style="color:#666">&gt;</span> <span style="color:#666">0</span>:
    limit_df <span style="color:#666">=</span> limit
<span style="color:#a2f;font-weight:bold">else</span>:
    limit_df <span style="color:#666">=</span> n_components

df_explained_variance_limited <span style="color:#666">=</span> df_explained_variance<span style="color:#666">.</span>iloc[:limit_df,:]

<span style="color:#080;font-style:italic">#make scree plot</span>
fig, ax1 <span style="color:#666">=</span> plt<span style="color:#666">.</span>subplots(figsize<span style="color:#666">=</span>(<span style="color:#666">15</span>,<span style="color:#666">6</span>))

ax1<span style="color:#666">.</span>set_title(<span style="color:#b44">&#39;Explained variance across principal components&#39;</span>, fontsize<span style="color:#666">=</span><span style="color:#666">14</span>)
ax1<span style="color:#666">.</span>set_xlabel(<span style="color:#b44">&#39;Principal component&#39;</span>, fontsize<span style="color:#666">=</span><span style="color:#666">12</span>)
ax1<span style="color:#666">.</span>set_ylabel(<span style="color:#b44">&#39;Explained variance&#39;</span>, fontsize<span style="color:#666">=</span><span style="color:#666">12</span>)

ax2 <span style="color:#666">=</span> sns<span style="color:#666">.</span>barplot(x<span style="color:#666">=</span>idx[:limit_df], y<span style="color:#666">=</span><span style="color:#b44">&#39;explained variance&#39;</span>, data<span style="color:#666">=</span>df_explained_variance_limited, palette<span style="color:#666">=</span><span style="color:#b44">&#39;summer&#39;</span>)
ax2 <span style="color:#666">=</span> ax1<span style="color:#666">.</span>twinx()
ax2<span style="color:#666">.</span>grid(False)

ax2<span style="color:#666">.</span>set_ylabel(<span style="color:#b44">&#39;Cumulative&#39;</span>, fontsize<span style="color:#666">=</span><span style="color:#666">14</span>)
ax2 <span style="color:#666">=</span> sns<span style="color:#666">.</span>lineplot(x<span style="color:#666">=</span>idx[:limit_df]<span style="color:#666">-</span><span style="color:#666">1</span>, y<span style="color:#666">=</span><span style="color:#b44">&#39;cumulative&#39;</span>, data<span style="color:#666">=</span>df_explained_variance_limited, color<span style="color:#666">=</span><span style="color:#b44">&#39;#fc8d59&#39;</span>)

ax1<span style="color:#666">.</span>axhline(mean_explained_variance, ls<span style="color:#666">=</span><span style="color:#b44">&#39;--&#39;</span>, color<span style="color:#666">=</span><span style="color:#b44">&#39;#fc8d59&#39;</span>) <span style="color:#080;font-style:italic">#plot mean</span>
ax1<span style="color:#666">.</span>text(<span style="color:#666">-.</span><span style="color:#666">8</span>, mean_explained_variance<span style="color:#666">+</span>(mean_explained_variance<span style="color:#666">*.</span><span style="color:#666">05</span>), <span style="color:#b44">&#34;average&#34;</span>, color<span style="color:#666">=</span><span style="color:#b44">&#39;#fc8d59&#39;</span>, fontsize<span style="color:#666">=</span><span style="color:#666">14</span>) <span style="color:#080;font-style:italic">#label y axis</span>

max_y1 <span style="color:#666">=</span> <span style="color:#a2f">max</span>(df_explained_variance_limited<span style="color:#666">.</span>iloc[:,<span style="color:#666">0</span>])
max_y2 <span style="color:#666">=</span> <span style="color:#a2f">max</span>(df_explained_variance_limited<span style="color:#666">.</span>iloc[:,<span style="color:#666">1</span>])
ax1<span style="color:#666">.</span>set(ylim<span style="color:#666">=</span>(<span style="color:#666">0</span>, max_y1<span style="color:#666">+</span>max_y1<span style="color:#666">*.</span><span style="color:#666">1</span>))
ax2<span style="color:#666">.</span>set(ylim<span style="color:#666">=</span>(<span style="color:#666">0</span>, max_y2<span style="color:#666">+</span>max_y2<span style="color:#666">*.</span><span style="color:#666">1</span>))

plt<span style="color:#666">.</span>show()
</code></pre></div><p><p class="markdown-image">
  <img src="/figures/pca_exploration/scree_plot.png" alt="Scree plot - Explained variance across components"  />
</p></p>
<p>A scree plot might show distinct jumps from one component to another. For example, when the first component captures disproportionately more variance than others, it could be a sign that variables inform about the same underlying factor or do not add additional dimensions, but say the same thing from a marginally different angle.</p>
<p>To give a direct example and to get a feeling for how distinct jumps might look like, I provide the scree plot of the <a 
    href="https://www.cs.toronto.edu/~delve/data/boston/bostonDetail.html"
    
    
     
      target="_blank" 
      rel="noopener"
    
>
    Boston house prices dataset
</a>:</p>
<p><p class="markdown-image">
  <img src="/figures/pca_exploration/scree_plot_boston.png" alt="Scree plot Boston housing prices dataset"  />
</p></p>
<h4 id="reasons-why-pca-saves-time-down-the-road">Reasons why PCA saves time down the road 

</h4><blockquote>
<p>Assume you have hundreds of variables, apply PCA and discover that over much of the explained variance is captured by the first few components. This might hint at a much lower number of underlying dimensions than the number of variables. Most likely, dropping some hundred variables leads to performance gains for training, validation and testing. There will be more time left to select a suitable model and refine it than to wait for the model itself to discover lack of variance behind several variables.</p>
</blockquote>
<blockquote>
<p>In addition to this, imagine that the data was constructed by oneself, e.g. through web scraping, and the scraper extracted pre-specified information from a web page. In that case, the retrieved information could be one-dimensional, when the developer of the scraper had only few relevant items in mind, but forgot to include items that shed light on further aspects of the problem setting. At this stage, it might be worthwhile to go back to the first step of the work flow and adjust data collection.</p>
</blockquote>
<h3 id="discover-underlying-factors-with-correlations-between-features-and-components">Discover underlying factors with correlations between features and components 

</h3><p>PCA offers another valuable statistic besides explained variance: The correlation between each principle component and a variable, also called factor loading. This statistic facilitates to grasp the dimension that lies behind a component. For example, a dataset includes information about individuals such as math score, reaction time and retention span. The overarching dimension would be cognitive skills and a component that strongly correlates with these variables can be interpreted as the cognitive skill dimension. Similarly, another dimension could be non-cognitive skills and personality, when the data has features such as self-confidence, patience or conscientiousness. A component that captures this area highly correlates with those features.</p>
<p>The following code creates a heatmap to inspect these correlations, also called factor loading matrix.</p>
<div class="highlight"><pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#080;font-style:italic"># adjust y-axis size dynamically</span>
size_yaxis <span style="color:#666">=</span> <span style="color:#a2f">round</span>(X_std<span style="color:#666">.</span>shape[<span style="color:#666">1</span>] <span style="color:#666">*</span> <span style="color:#666">0.5</span>)
fig, ax <span style="color:#666">=</span> plt<span style="color:#666">.</span>subplots(figsize<span style="color:#666">=</span>(<span style="color:#666">8</span>,size_yaxis))

<span style="color:#080;font-style:italic"># plot the first top_pc components</span>
top_pc <span style="color:#666">=</span> <span style="color:#666">3</span>
sns<span style="color:#666">.</span>heatmap(df_c<span style="color:#666">.</span>iloc[:,:top_pc], annot<span style="color:#666">=</span>True, cmap<span style="color:#666">=</span><span style="color:#b44">&#34;YlGnBu&#34;</span>, ax<span style="color:#666">=</span>ax)
plt<span style="color:#666">.</span>show()
</code></pre></div><p><p class="markdown-image">
  <img src="/figures/pca_exploration/heatmap.png" alt="Factor loadings - Correlations of variables and components"  />
</p></p>
<p>The first component strongly negatively associates with task ability, reasoning score (raven), math score, verbal score and positively links to beliefs about being gritty (grit_survey1). Summarizing this into a common underlying factor is subjective and requires domain knowledge. In my opinion, the first component mainly captures cognitive skills.</p>
<p>The second component correlates negatively with receiving the treatment (grit), gender (male) and positively relates to being inconsistent. Interpreting this dimension is less clear-cut and much more challenging. Nevertheless, it accounts for 12% of explained variance instead of 27% like the first component, which results in less interpretable dimensions as it spans slightly across several topical areas. All components that follow might be analogously difficult to interpret.</p>
<p>Evidence that variables capture similar dimensions could be uniformly distributed factor loadings. One example which inspired this article is a project of mine with Google Trends and self-constructed keywords about a firm&rsquo;s sustainability (<a href="https://philippschmalen.github.io/posts/esg_scores_pytorch_googletrends">https://philippschmalen.github.io/posts/esg_scores_pytorch_googletrends</a> ). A list of the 15th highest factor loadings for the first principle component revealed loadings ranging from 0.12 as the highest value to 0.11 as the lowest loading of all 15. Such a uniform distribution of factor loadings could be an issue. This especially applies when data is self-collected and someone preselected what is being considered for collection. Adjusting this selection might add dimensionality to your data which possibly improves model performance at the end.</p>
<h4 id="another-reason-why-pca-saves-time-down-the-road">Another reason why PCA saves time down the road 

</h4><blockquote>
<p>If the data was self-constructed, the factor loadings show how each feature contributes to an underlying dimension, which helps to come up with additional perspectives on data collection and what features or dimensions could add valuable variance. Rather than blind guessing which features to add, factor loadings lead to informed decisions for data collection. They may even be an inspiration in the search for more advanced features.</p>
</blockquote>
<h2 id="conclusion">Conclusion 

</h2><p>All in all, PCA is a flexible instrument in the toolbox for data exploration. Its main purpose is to reduce complexity of large datasets. But it also serves well to look beneath the surface of variables, discover latent dimensions and relate variables to these dimensions, making them interpretable. Key metrics to inspect are <strong>explained variance</strong> and <strong>factor loading</strong>.</p>
<p>This article shows how to leverage these metrics for data exploration that goes beyond averages, distributions and correlations and build an understanding underlying properties of the data. Identifying patterns across variables is valuable to rethink previous steps in the project work flow, such as data collection, processing or feature engineering.</p>
<blockquote>
<p>Thanks for reading! I hope you find it as useful as I had fun to write this guide. I am curious of your thoughts on this matter. If you have any feedback I highly appreciate your feedback and look forward receiving your message.</p>
</blockquote>
<h2 id="appendix">Appendix 

</h2><h3 id="access-the-jupyter-notebook">Access the Jupyter Notebook 

</h3><p>I applied PCA to even more exemplary datasets like Boston housing market, wine and iris using <code>do_pca()</code>. It illustrates how PCA output looks like for small datasets. Feel free to download <a 
    href="https://github.com/philippschmalen/Code-snippets/blob/master/PCA_workflow.ipynb"
    
    
     
      target="_blank" 
      rel="noopener"
    
>
    my notebook
</a> or <a 
    href="https://github.com/philippschmalen/Code-snippets/blob/master/helper_pca.py"
    
    
     
      target="_blank" 
      rel="noopener"
    
>
    script
</a>.</p>
<h3 id="note-on-factor-analysis-vs-pca">Note on factor analysis vs. PCA 

</h3><p>A rule of thumb formulated <a 
    href="https://stats.stackexchange.com/a/1579"
    
    
     
      target="_blank" 
      rel="noopener"
    
>
    here
</a> states: Use PCA if you want to reduce your correlated observed variables to a smaller set of uncorrelated variables and use factor analysis to test a model of latent factors on observed variables.</p>
<p>Even though, this distinction is scientifically correct, it becomes less relevant in an applied context. PCA relates closely to factor analysis which often leads to similar conclusions about data properties which is what we care about. Therefore, the distinction can relaxed for data exploration. <a 
    href="https://stats.stackexchange.com/a/133806"
    
    
     
      target="_blank" 
      rel="noopener"
    
>
    This post
</a> gives an example in an applied context and another example with hands-on code for factor analysis is attached in the <a 
    href="https://github.com/philippschmalen/Code-snippets/blob/master/PCA_workflow.ipynb"
    
    
     
      target="_blank" 
      rel="noopener"
    
>
    notebook
</a>.</p>
<p>Finally, for those interested about the differences between factor analysis and PCA refer to <a 
    href="https://stats.stackexchange.com/questions/123063/is-there-any-good-reason-to-use-pca-instead-of-efa-also-can-pca-be-a-substitut"
    
    
     
      target="_blank" 
      rel="noopener"
    
>
    this post
</a>. Note, that throughout this article I never used the term latent factor to be precise.</p>
<h3 id="references">References 

</h3><p>[1] Alan, S., Boneva, T., &amp; Ertac, S. (2019). Ever failed, try again, succeed better: Results from a randomized educational intervention on grit. The Quarterly Journal of Economics, 134(3), 1121-1162.</p>
<p>[2] Smith, L. I. (2002). A tutorial on principal components analysis.</p>

    </div>

    
        <div class="tags">
            
                <a href="https://philippschmalen.github.io/tags/pca">PCA</a>
            
                <a href="https://philippschmalen.github.io/tags/data-exploration">Data Exploration</a>
            
                <a href="https://philippschmalen.github.io/tags/hidden-factors">Hidden Factors</a>
            
                <a href="https://philippschmalen.github.io/tags/dimensionality">Dimensionality</a>
            
                <a href="https://philippschmalen.github.io/tags/toolbox">Toolbox</a>
            
        </div>
    
    
    

</section>


    </main>
    
    
<footer id="footer">
    
        <div id="social">


    <a class="inline-svg" href="mailto:philippschmalen@gmail.com" target="_blank" height="28" width="28">
        
        <svg aria-hidden="true" focusable="false" data-prefix="far" data-icon="envelope" class="svg-inline--fa fa-envelope fa-w-16" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M464 64H48C21.49 64 0 85.49 0 112v288c0 26.51 21.49 48 48 48h416c26.51 0 48-21.49 48-48V112c0-26.51-21.49-48-48-48zm0 48v40.805c-22.422 18.259-58.168 46.651-134.587 106.49-16.841 13.247-50.201 45.072-73.413 44.701-23.208.375-56.579-31.459-73.413-44.701C106.18 199.465 70.425 171.067 48 152.805V112h416zM48 400V214.398c22.914 18.251 55.409 43.862 104.938 82.646 21.857 17.205 60.134 55.186 103.062 54.955 42.717.231 80.509-37.199 103.053-54.947 49.528-38.783 82.032-64.401 104.947-82.653V400H48z"></path></svg>
    </a>

    <a class="inline-svg" href="https://github.com/philippschmalen" target="_blank" height="28" width="28">
        
        <svg aria-hidden="true" focusable="false" data-prefix="fab" data-icon="github" class="svg-inline--fa fa-github fa-w-16" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><path fill="currentColor" d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"></path></svg>
    </a>

    <a class="inline-svg" href="https://www.linkedin.com/in/philippschmalen/" target="_blank" height="28" width="28">
        
        <svg aria-hidden="true" focusable="false" data-prefix="fab" data-icon="linkedin-in" class="svg-inline--fa fa-linkedin-in fa-w-14" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path fill="currentColor" d="M100.28 448H7.4V148.9h92.88zM53.79 108.1C24.09 108.1 0 83.5 0 53.8a53.79 53.79 0 0 1 107.58 0c0 29.7-24.1 54.3-53.79 54.3zM447.9 448h-92.68V302.4c0-34.7-.7-79.2-48.29-79.2-48.29 0-55.69 37.7-55.69 76.7V448h-92.78V148.9h89.08v40.8h1.3c12.4-23.5 42.69-48.3 87.88-48.3 94 0 111.28 61.9 111.28 142.3V448z"></path></svg>
    </a>

</div>




    

    <p class="copyright">
    
       © Copyright 
       2021 
       Philipp Schmalen
    
    </p>
</footer>

<script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.9.0/katex.min.js" integrity="sha256-34ADEQM6cIZ7chSRA07lN4aD5JM9IQoeIr2VamKDcT0=" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.9.0/contrib/auto-render.min.js" integrity="sha256-HkMrKMLKQk4t1R2ofMAcLz72fWM2sshnx6215U+LgU0=" crossorigin="anonymous"></script>
<script>
  renderMathInElement(document.body,
    {
        delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "$", right: "$", display: false},
        ]
    }
  );

  var inlineMathArray = document.querySelectorAll("script[type='math/tex']");
  for (var i = 0; i < inlineMathArray.length; i++) {
    var inlineMath = inlineMathArray[i];
    var tex = inlineMath.innerText || inlineMath.textContent;
    var replaced = document.createElement("span");
    replaced.innerHTML = katex.renderToString(tex, {displayMode: false});
    inlineMath.parentNode.replaceChild(replaced, inlineMath);
  }

  var displayMathArray = document.querySelectorAll("script[type='math/tex; mode=display']");
  for (var i = 0; i < displayMathArray.length; i++) {
    var displayMath = displayMathArray[i];
    var tex = displayMath.innerHTML;
    var replaced = document.createElement("span");
    replaced.innerHTML = katex.renderToString(tex.replace(/%.*/g, ''), {displayMode: true});
    displayMath.parentNode.replaceChild(replaced, displayMath);
  }
</script>
  </body>
</html>
