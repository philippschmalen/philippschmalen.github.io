<!DOCTYPE html>
<html lang="en-us">
  <head>
    <title>Predicting ESG risks with Pytorch, Google Trends and Amazon SageMaker | Philipp Schmalen</title>

    <meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">    
<meta name="viewport" content="width=device-width,minimum-scale=1">
<meta name="description" content="with windows subsystem for Linux (WSL)">
<meta name="generator" content="Hugo 0.80.0" />


  <META NAME="ROBOTS" CONTENT="NOINDEX, NOFOLLOW">


<link rel="stylesheet" href="/css/style.css">
<link rel="shortcut icon" href="/images/favicon.ico" type="image/x-icon" />

 
    
<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
	ga('create', 'UA-154596617-1', 'auto');
	
	ga('send', 'pageview');
}
</script>
<script async src='https://www.google-analytics.com/analytics.js'></script>








  </head>

  <body>
    <nav class="navigation">
	
		<a href="/"> <span class="arrow">←</span>Home</a>
	
	
	

	

	<a href="/about">About</a>

	

	
</nav>


    <main class="main">
      

<section id="single">
    <h1 class="title">Predicting ESG risks with Pytorch, Google Trends and Amazon SageMaker</h1>

    <div class="tip">
        <span>
          Aug 7, 2020
        </span>
        <span class="split">
          ·
        </span>

        <span>
          20 minute read
        </span>
    </div>

    <div class="content">
      <h1 id="predicting-esg-risks-with-pytorch-google-trends-and-amazon-sagemaker">Predicting ESG risks with Pytorch, Google Trends and Amazon SageMaker 

</h1><!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<h2 id="introduction">Introduction 

</h2><!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<p>The project deals with a prediction task within finance, and in particular, sustainable investment. Private investors increasingly demand firms that act socially responsible, environmentally friendly and have good governance. The UN and leading asset management firms developed the three pillars of ESG, namely environmental, social and governmental, in a conference in 2005 (International Finance Corporation et al., 2005).</p>
<p>A few years ago investors were willing to accept lower returns for socially responsible investments, but firms with higher ESG scores closed the gap. Auer &amp; Schuhmacher (2016) do not find lower returns of ESG investing compared to a market portfolio. This gives way to an increased demand by investors to invest according the ESG pillars and favor firms that score higher in ESG ratings. FTSE Russell (2018) finds that more than half of global asset owners are currently implementing or evaluating ESG considerations in their investment strategy.</p>
<p>ESG becomes increasingly relevant in today&rsquo;s investment decisions, especially in the face of global challenges such as climate change, human rights abuse or gender equality. That is why I want to find out more about ESG scores and how they can be predicted for firms using non-primary data from Google Trends.</p>
<h3 id="problem-statement">Problem Statement 

</h3><!-- raw HTML omitted -->
<p>ESG investing requires individual investors to be disproportionately well informed. A lot of data about a firm has to be collected, processed and interpreted which leads to a high workload and cognitive strain.</p>
<p>A fundamental problem lies in conflict of interest of the firm to not disclose anything which leads to bad press. Reports on sustainability, corporate social responsibility and even financial reports are often subject to manipulation at worst or overly positive framing at best. It is hard to distinguish between a company that is truly transparent and lives up to its self-defined sustainability principles or just pretends to act accordingly. To break this information asymmetry it often involves third parties, such as investigative journalism, whistle blowing or tight regulation.</p>
<p>Overall ESG scores and ratings can be a starting point for ESG investing and provide a shortcut. Investors rely on different approaches to construct ESG compliant portfolios. One such approach excludes firms that fulfill negative criteria, such as being badly governed, involvement in scandals or even reliance on fossil fuels.</p>
<p>Still, some investors want to be informed in detail, but their hunger for transparency is not met by the firm, which wants to hide and obscure scandals, visible in the media. A firm experiencing a scandal is salient to the public, why it likely appears more frequently in Google searches. Furthermore, providers of publicly available ESG ratings might not update their scores as frequently as professional paid services. Google trends can indicate short-term sentiment against a firm when negative criteria spike in search frequency.</p>
<p>Another factor is about the complexity of ESG ratings. There is no standardized or regulated way to calculate ESG metrics, which leads to multiple methodologies to construct them. Each provider sells their methodology as superior, while details sometimes remain obscure. This is especially the case when sentiment analyses are included which involves intricate data collection, processing and modeling, which is too much to digest for an investor who tries to fully understand how they derive ESG metrics. In contrast to this, relying on Google searches is straightforward to understand and reduces complexity.</p>
<p>Therefore, the project investigates the potential of using Google trends to infer ESG scores and focuses on the main question:</p>
<hr>
<p><strong>Can Google trends and machine learning inform investors about a firm&rsquo;s ESG performance?</strong></p>
<hr>
<p>A more technical version of the problem statement could be: <em>Can machine learning models classify firms in their ESG performance based on Google search frequency?</em></p>
<h3 id="main-results">Main results 

</h3><p>The answer the these questions can be summarized in three main results:</p>
<ol>
<li>Gathering data through Google trends is time consuming and works without errors with 20 seconds timeout after each keyword query</li>
<li>Negative keywords for ESG criteria positively correlate with each other and share one underlying latent factor that accounts most for the explained variance</li>
<li>The neural network predicts slightly better than a random guess, which likely stems from limitations of the data</li>
</ol>
<p>The analysis concludes with an outlook on follow-up projects. Future work should focus on extensive datasets with features that also include positive screening keywords</p>
<h3 id="implementation-roadmap">Implementation roadmap 

</h3><p>To arrive at the results and structure my work, I followed a detailed implementation plan.</p>
<ol>
<li>Data collection
<ol>
<li>Get tickers from S&amp;P 500 on Wikipedia</li>
<li>Get main outcome: ESG risk, obtained through the yahooquery Pypi package (accessed with the parameter esg_scores for a ticker)</li>
<li>Obtain search metrics on ESG related keywords from Google trends through the pytrends Pypi package </li>
</ol>
</li>
<li>Data processing and feature transformation
<ol>
<li>Collapse time dimension of Google trends search index into the following metrics for each keyword, using a defined time span, such as one year</li>
<li>Average search index across one year</li>
<li>apply an exponential decay function as a weight, assigning higher weight to the most recent year</li>
<li>create binary classifier based on median split on ESG score, indicating high and low ESG performers</li>
<li>Re-shape dataset into wide format, with columns as features and rows as firms</li>
<li>Split data into a train and test set, convert to .csv and upload to S3</li>
</ol>
</li>
<li>Descriptive statistics
<ol>
<li>List top-5 means of some variables</li>
<li>Correlation heatmap</li>
<li>inspect the outcome variables</li>
</ol>
</li>
<li>Training, validating and testing a model with Sagemaker
<ol>
<li>Write scripts for logistic regression benchmark: train.py</li>
<li>Write scripts for PyTorch neural network: model.py and train.py</li>
<li>Instantiate estimators with Sagemaker </li>
<li>Run training job</li>
<li>Deploy models for testing</li>
</ol>
</li>
<li>Evaluation and benchmark comparison 
<ol>
<li>Key metrics for model performance: accuracy, precision and recall</li>
<li>Possible model adjustment of the neural net when it lacks precision</li>
</ol>
</li>
<li>Clean up resources
<ol>
<li>Delete endpoint</li>
<li>Remove other resources, such as emptying S3 bucket training jobs, endpoint configurations, notebook instances</li>
</ol>
</li>
</ol>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<h2 id="data">Data 

</h2><!-- raw HTML omitted -->
<p>The following sections describe how I construct the dataset from scratch. First, Google trends is introduced and how keywords are composed. Second, the Pytrends library is described along with challenges imposed by Google&rsquo;s unknown rate limits as well as the metric of search interest over time.</p>
<!-- raw HTML omitted -->
<h3 id="constructing-the-dataset">Constructing the dataset 

</h3><p>Google trends (<a href="https://trends.google.com/trends/?geo=US">https://trends.google.com/trends/?geo=US</a>) shows search interest for a given keyword within a specified region. As a starting point, I deal with American firms and thus focus on the US using English keywords. 
The Google trend indicator quantifies the relative search volume of searches between two or more terms.</p>
<p>Relative search volume could be interpreted as search interest and ranges from 0 to 100. To be clear, it lacks a defined measurement unit such as an absolute search count, but relates to all other keywords that were part of the query to Google trends.</p>
<p>A popular procedure to select ESG investments is by checking on whether a firm fulfills defined exclusion criteria, which is termed negative screening. For example, if a firm engages in  arms trade, it would be excluded from ESG portfolios. Some other examples are firms  linked to corruption, relying on fossil fuels or notorious for avoiding taxes. On the opposite, another ESG investment approach would be positive screening, where a person selects firms since they rely on renewables, promote gender equality or score high in transparency.</p>
<p>I constructed 30 keywords based on negative screening, since issues about a particular firm likely appear in the news and are therefore more salient to the public than a corporate social responsiblity project mentioned in a sustainability report. I chose them in a way to cover a broad range of relevant topics for negative screening and derived them in part from negating UN&rsquo;s sustainable development goals (<a href="https://www.un.org/sustainabledevelopment/sustainable-development-goals/)">https://www.un.org/sustainabledevelopment/sustainable-development-goals/)</a>. Additionally, I included general keywords that people will search for, when they suspect a firm acting against their morale, such as &ldquo;firm xy issues&rdquo; or &ldquo;firm xy bad&rdquo;. Keywords like &ldquo;scandal&rdquo; or &ldquo;lawsuit&rdquo; will possibly cover a lot of specific terminology used by ESG investors for negative screening, but are assumed to be more common among individuals.</p>
<p>The overarching themes within my keywords inlcude ecological impact, gender equality, activities against law, negative news coverage, negative public image and weapons. All in all, I composed 30 keywords to capture a broad range of negative screening criteria and included the following keywords:</p>
<pre><code>'scandal', 'greenwashing', 'corruption', 'fraud', 'bribe', 'tax',
 'forced', 'harassment', 'violation', 'human rights', 'conflict', 
 'weapons', 'arms trade', 'pollution', 'CO2', 'emission', 'fossil 
 fuel','gender inequality', 'discrimination', 'sexism', 'racist', 
 'intransparent', 'data privacy', 'lawsuit', 'unfair', 'bad', 'problem', 
 'hate', 'issues', 'controversial'
</code></pre>
<p>The topical keywords need to be merged with firm names to end up with keywords that are passed to Google trends. To achieve this, I merge the topic keywords with firm names and get topic-firm pairwise combinations, such as</p>
<pre><code>'scandal 3M ', 'greenwashing 3M ', 'corruption 3M ', 'fraud 3M 'or 'bribe 3M '
</code></pre>
<h3 id="using-pytrends-at-scale-for-google-trends">Using Pytrends at scale for Google Trends 

</h3><p>To access data from Google trends and establish a connection, the project relies on the Pytrends package. 
Several issues arise from gathering data with Google trends. Firstly, Google trends allows a maximum of five keywords per query. Secondly, a rate limit exists which protects Google&rsquo;s servers from too many requests. Thirdly, the metric of search interest is scaled based on the search interest of the keyword with the most searches as compared to other keywords. 
The query limit requires a simple workaround, where I need to batch keywords and subsequently pass each keyword batch to Google trends. With $500$ firms, $30$ keywords and a batch size of $5$, there are $500 \times 30\times \frac{1}{5}=3000$ queries.</p>
<p>The rate limit calls for a timeout. Since Google does not publish information about their search backend due to secutiry reasons, the exact rate limit is unknown. But, query errors lead to missing data entries, which leads to fewer data points to train, validate or test the model. Therefore, I favor a conservative approach, setting the timeout to 20 seconds. The downside of this are runtimes of $3000 \times 20 $ seconds $= 60.000$ seconds $\approx 16.7$ hours. A distinct jupyter notebook that exectutes queries in the background and stores query responses in a csv in case of error response, frees up coding capacity for model setup and avoids data loss caused by rate limits.</p>
<p>The last challenge with Google trends is the relative search metric. To exemplify this, a query of three words with [&lsquo;pizza&rsquo;, &lsquo;president&rsquo;, &lsquo;covid&rsquo;] scales the search interest to the keyword with the most number of searches. The images below illustrate this. It would be a cause of concern when keywords belong to distinct informational sets. Nevertheless, the constructed keywords have the firm name in common, which makes outliers in search interest across queries unlikely since they have the firm name in common. To be concice, I assume that the firm name sufficiently conncets keywords and thereby reduces outliers caused by the relative metric.</p>
<p>Below is the main code snippet, illustrating the timeout function and querying Google trends in a way to not raise any exceptions.</p>
<div class="highlight"><pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">
<span style="color:#080;font-style:italic"># PYTREND HELPERS</span>
<span style="color:#a2f;font-weight:bold">def</span> <span style="color:#00a000">pytrends_sleep_init</span>(seconds):
    <span style="color:#b44">&#34;&#34;&#34;Timeout for certain seconds and re-initialize pytrends
</span><span style="color:#b44">    
</span><span style="color:#b44">    Input
</span><span style="color:#b44">        seconds: int with seconds for timeout
</span><span style="color:#b44">        
</span><span style="color:#b44">    Return
</span><span style="color:#b44">        None
</span><span style="color:#b44">    
</span><span style="color:#b44">    &#34;&#34;&#34;</span>
    <span style="color:#a2f;font-weight:bold">print</span>(<span style="color:#b44">&#34;TIMEOUT for {} sec.&#34;</span><span style="color:#666">.</span>format(seconds))
    sleep(seconds)
    pt <span style="color:#666">=</span> TrendReq()


<span style="color:#080;font-style:italic"># Define timeout </span>
sec_sleep <span style="color:#666">=</span> <span style="color:#666">20</span>


<span style="color:#080;font-style:italic"># initialize pytrends</span>
pt <span style="color:#666">=</span> TrendReq()

<span style="color:#080;font-style:italic"># store DFs for later concat</span>
df_list <span style="color:#666">=</span> []
index_batch_error <span style="color:#666">=</span> []

<span style="color:#080;font-style:italic"># create csv to store intermediate results</span>
make_csv(pd<span style="color:#666">.</span>DataFrame(), filename<span style="color:#666">=</span><span style="color:#b44">&#39;googletrends.csv&#39;</span>, data_dir<span style="color:#666">=</span><span style="color:#b44">&#39;data&#39;</span>, append<span style="color:#666">=</span>False)

<span style="color:#a2f;font-weight:bold">for</span> i, batch <span style="color:#a2f;font-weight:bold">in</span> <span style="color:#a2f">enumerate</span>(keyword_batches):
    
    <span style="color:#080;font-style:italic"># retrieve interest over time</span>
    <span style="color:#a2f;font-weight:bold">try</span>:
        <span style="color:#080;font-style:italic"># re-init pytrends and wait (sleep/timeout)</span>
        pytrends_sleep_init(sec_sleep)
        
        <span style="color:#080;font-style:italic"># pass keywords to pytrends API</span>
        pt<span style="color:#666">.</span>build_payload(kw_list<span style="color:#666">=</span>batch) 
        <span style="color:#a2f;font-weight:bold">print</span>(<span style="color:#b44">&#34;Payload build for {}. batch&#34;</span><span style="color:#666">.</span>format(i))
        df_search_result <span style="color:#666">=</span> pt<span style="color:#666">.</span>interest_over_time()
        
    <span style="color:#a2f;font-weight:bold">except</span> <span style="color:#d2413a;font-weight:bold">Exception</span> <span style="color:#a2f;font-weight:bold">as</span> e:
        <span style="color:#a2f;font-weight:bold">print</span>(e)
        <span style="color:#a2f;font-weight:bold">print</span>(<span style="color:#b44">&#34;Query {} of {}&#34;</span><span style="color:#666">.</span>format(i, n_query))
        <span style="color:#080;font-style:italic"># store index at which error occurred</span>
        index_batch_error<span style="color:#666">.</span>append(i)
        
        <span style="color:#080;font-style:italic"># re-init pytrends and wait (sleep/timeout)</span>
        pytrends_sleep_init(sec_sleep)
        
        <span style="color:#080;font-style:italic"># retry</span>
        <span style="color:#a2f;font-weight:bold">print</span>(<span style="color:#b44">&#34;RETRY for {}. batch&#34;</span><span style="color:#666">.</span>format(i))
        pt<span style="color:#666">.</span>build_payload(kw_list<span style="color:#666">=</span>batch) 
        df_search_result <span style="color:#666">=</span> pt<span style="color:#666">.</span>interest_over_time()
        
    <span style="color:#080;font-style:italic"># check for non-empty df</span>
    <span style="color:#a2f;font-weight:bold">if</span> df_search_result<span style="color:#666">.</span>shape[<span style="color:#666">0</span>] <span style="color:#666">!=</span> <span style="color:#666">0</span>:
        
        <span style="color:#080;font-style:italic"># reset index for consistency (to call pd.concat later with empty dfs)</span>
        df_search_result<span style="color:#666">.</span>reset_index(inplace<span style="color:#666">=</span>True)
        df_list<span style="color:#666">.</span>append(df_search_result)
        
    <span style="color:#080;font-style:italic"># no search result for any keyword</span>
    <span style="color:#a2f;font-weight:bold">else</span>:        
        <span style="color:#080;font-style:italic"># create df containing 0s</span>
        df_search_result <span style="color:#666">=</span> pd<span style="color:#666">.</span>DataFrame(np<span style="color:#666">.</span>zeros((<span style="color:#666">261</span>,batch_size)), columns<span style="color:#666">=</span>batch)
        df_list<span style="color:#666">.</span>append(df_search_result)
        
    make_csv(df_search_result, filename<span style="color:#666">=</span><span style="color:#b44">&#39;googletrends.csv&#39;</span>, data_dir<span style="color:#666">=</span><span style="color:#b44">&#39;data&#39;</span>,
             append<span style="color:#666">=</span>True,
            header<span style="color:#666">=</span>True)

</code></pre></div><!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<h3 id="preprocessing-and-feature-engineering">Preprocessing and feature engineering 

</h3><p>To feed the data into the model, it has to meet certain criteria. A simple data format has rows for individuals and columns for an individual&rsquo;s characteristics. In this example, it should be a matrix where one row stands for one firm and where columns contain information about the firm.</p>
<p>However, Google trends returns a time-series for each keyword spanning the last five years, reported weekly. This amounts to $260$ ($52$ weeks  $\times$ $5$ years  $= 260$) entries for each keyword and yields the so-called &ldquo;long&rdquo; data format for a given firm, A and 30 search keyword:</p>
<table>
<thead>
<tr>
<th>Firm</th>
<th>time</th>
<th>keyword_1</th>
<th>&hellip;</th>
<th>keyword_30</th>
</tr>
</thead>
<tbody>
<tr>
<td>A</td>
<td>1</td>
<td>25</td>
<td>&hellip;</td>
<td>30</td>
</tr>
<tr>
<td>A</td>
<td>2</td>
<td>25</td>
<td>&hellip;</td>
<td>30</td>
</tr>
<tr>
<td>&hellip;</td>
<td>&hellip;</td>
<td>&hellip;</td>
<td>&hellip;</td>
<td>&hellip;</td>
</tr>
<tr>
<td>A</td>
<td>260</td>
<td>25</td>
<td>&hellip;</td>
<td>30</td>
</tr>
</tbody>
</table>
<p>Thus, the data needs to be re-shaped into a wide format as previously described. 
Ideally, re-shaping does not imply information loss. Nevertheless, weekly data might be too granular for the prediction task and likely does not add valuable information as compared to year averages. Thus, taking yearly averages is assumed to have sufficient information about a topic. Peaks in search volume are mirrored by higher year averages instead of week-to-week differences.
Averaging by year, adding the time dimension to keywords variables and thereby collapsing the time dimension, yields the wide format. The following table illustrates the desired wide format, which can be fed into the model:</p>
<table>
<thead>
<tr>
<th>Firm</th>
<th>keyword_1_t1</th>
<th>keyword_1_t2</th>
<th>&hellip;</th>
<th>keyword_30_t5</th>
</tr>
</thead>
<tbody>
<tr>
<td>A</td>
<td>0</td>
<td>2</td>
<td>&hellip;</td>
<td>50</td>
</tr>
<tr>
<td>B</td>
<td>3</td>
<td>25</td>
<td>&hellip;</td>
<td>77</td>
</tr>
<tr>
<td>&hellip;</td>
<td>&hellip;</td>
<td>&hellip;</td>
<td>&hellip;</td>
<td>&hellip;</td>
</tr>
<tr>
<td>Z</td>
<td>14</td>
<td>60</td>
<td>&hellip;</td>
<td>42</td>
</tr>
</tbody>
</table>
<p><p class="markdown-image">
  <img src="/figures/esg_googletrends/gtrends_ex1.png" alt=""  />
</p>
<p class="markdown-image">
  <img src="/figures/esg_googletrends/gtrends_ex2.png" alt=""  />
</p></p>
<h3 id="overview-of-the-final-dataset">Overview of the final dataset 

</h3><p>The dataset is a $N\times(M+1)$ matrix which contains overall $N=305$ rows, $i$, with $M=145$ features, $X_m$, and one outcome variable, $y$. The $145$ features, $X_m$, indicate year-average search interest for a keyword-firm combination at time $t$, which spans five years, indicated by the variable suffixes $0$ to $4$.</p>
<p>The outcome variable, $y$, is a firm&rsquo;s overall ESG score. Which is later used to construct the binary ESG score high/low variable through a median split. Besides this binary variable, all other metrics count as a ordinal attribute types. They are ordered, while their relative magnitude has no physical meaning. ESG scores rank firms relatively within sector, so that a one point difference for one firm does not mean the same for another. The same holds for relative search interest for a given keyword-firm pair.</p>
<h3 id="descriptive-statistics">Descriptive statistics 

</h3><h4 id="variable-means">Variable means 

</h4><p>I focus on the most recent period, corresponding to the time window from today to 1 year ago. The variables have suffix $_4$ and were not downward weighted by the decay function, which means we see the unprocessed metric for search interest. Looking at top-five highest and lowest variable means yields the following tables.</p>
<table>
<thead>
<tr>
<th>Variable</th>
<th>5 highest means</th>
</tr>
</thead>
<tbody>
<tr>
<td>tax_4</td>
<td>14.731021</td>
</tr>
<tr>
<td>bad_4</td>
<td>13.299369</td>
</tr>
<tr>
<td>problem_4</td>
<td>4.437894</td>
</tr>
<tr>
<td>issues_4</td>
<td>4.334615</td>
</tr>
<tr>
<td>fraud_4</td>
<td>4.154477</td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr>
<th>Variable</th>
<th>5 lowest means</th>
</tr>
</thead>
<tbody>
<tr>
<td>harassment_4</td>
<td>0.103090</td>
</tr>
<tr>
<td>bribe_4</td>
<td>0.001955</td>
</tr>
<tr>
<td>greenwashing_4</td>
<td>0.000000</td>
</tr>
<tr>
<td>intransparent_4</td>
<td>0.000000</td>
</tr>
<tr>
<td>arms trade_4</td>
<td>0.000000</td>
</tr>
</tbody>
</table>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<h4 id="correlations">Correlations 

</h4><p>The correlation heatmap depicts a uniform pattern of rather strong positive correlation around .25. Additionally, it makes variables with zero variance and means salient. Normally, we would like to see more intricate correlations and uniform patterns such as these are uncommon. However, this is due to choice of keywords, which all relate to negative criteria within the ESG setting. Since they share this overarching theme, the uniform pattern becomes explainable and motivate other approaches to include more diverse keywords that also include ESG topics of positive screening.</p>
<p><p class="markdown-image">
  <img src="/figures/esg_googletrends/corr_heatmap.png" alt="Correlation heatmap"  />
</p></p>
<h4 id="distribution-of-the-outcome-variable">Distribution of the outcome variable 

</h4><p>Below is the distribution of ESG scores on which the median split is based. It shows a few outliers to the right, but seems evenly distributed. It shows that a median split is a rather safe clean way to divide the data.</p>
<p><p class="markdown-image">
  <img src="/figures/esg_googletrends/ESG_score_y.png" alt="ESG scores distribution"  />
</p></p>
<!-- raw HTML omitted -->
<h3 id="principle-component-analysis-pca">Principle component analysis (PCA) 

</h3><h4 id="explained-variance">Explained variance 

</h4><p>As an additional part of descriptive statistics, I conduct a principle component analysis (PCA). Plotting explained variance of each component indicates how many latent features are present in the data. Both the scree plot and the table below show a sharp decline of explained variance from the first to the second component, where the latter accounts for less than 1/4 explained variance of the first component. This might hint at a latent factor across multiple features and a larger interdependence among them, as similarly indicated by the correlation pattern.</p>
<p><p class="markdown-image">
  <img src="/figures/esg_googletrends/pca_explained_var.png" alt=""  />
</p></p>
<table>
<thead>
<tr>
<th>PC</th>
<th>explained variance</th>
<th>cumulative</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>0.386840</td>
<td>0.386840</td>
</tr>
<tr>
<td>2</td>
<td>0.091858</td>
<td>0.478698</td>
</tr>
<tr>
<td>3</td>
<td>0.066203</td>
<td>0.544901</td>
</tr>
<tr>
<td>4</td>
<td>0.056188</td>
<td>0.601089</td>
</tr>
<tr>
<td>5</td>
<td>0.048013</td>
<td>0.649102</td>
</tr>
</tbody>
</table>
<p>To further pin down the hypothesis about latent factors, factor loadings can be examined. In some cases, factor loadings reveal a common pattern, where in other cases, it cannot be easily interpreted when it does not match into a narrative. Therefore, the next section deals with factor loadings.</p>
<h4 id="factor-loadings">Factor loadings 

</h4><p>Below is a table showing the top-3 highest and lowest factor loadings for the first principle component.</p>
<table>
<thead>
<tr>
<th>PC 1</th>
<th>lowest  factor loadings</th>
</tr>
</thead>
<tbody>
<tr>
<td>arms trade_0</td>
<td>-1.110223e-16</td>
</tr>
<tr>
<td>greenwashing_1</td>
<td>-0.000000e+00</td>
</tr>
<tr>
<td>intransparent_4</td>
<td>-0.000000e+00</td>
</tr>
</tbody>
</table>
<p>The lowest coefficients show zero connection to the keywords arms trade, greenwashing and intransparent. Referring to the mean statistics above explains these low values, since these variables do not have any variance, being a constant 0.</p>
<table>
<thead>
<tr>
<th>PC1</th>
<th>highest factor loadings</th>
</tr>
</thead>
<tbody>
<tr>
<td>conflict_2</td>
<td>0.122832</td>
</tr>
<tr>
<td>conflict_0</td>
<td>0.121809</td>
</tr>
<tr>
<td>conflict_1</td>
<td>0.121395</td>
</tr>
<tr>
<td>conflict_3</td>
<td>0.120161</td>
</tr>
<tr>
<td>conflict_4</td>
<td>0.119546</td>
</tr>
<tr>
<td>weapons_2</td>
<td>0.118173</td>
</tr>
<tr>
<td>weapons_1</td>
<td>0.117959</td>
</tr>
<tr>
<td>weapons_3</td>
<td>0.117439</td>
</tr>
<tr>
<td>weapons_0</td>
<td>0.116779</td>
</tr>
<tr>
<td>weapons_4</td>
<td>0.115938</td>
</tr>
<tr>
<td>discrimination_2</td>
<td>0.112915</td>
</tr>
<tr>
<td>discrimination_0</td>
<td>0.112702</td>
</tr>
<tr>
<td>discrimination_4</td>
<td>0.112656</td>
</tr>
<tr>
<td>discrimination_1</td>
<td>0.112234</td>
</tr>
<tr>
<td>discrimination_3</td>
<td>0.112179</td>
</tr>
</tbody>
</table>
<p>Inspecting the highest factor loadings uncovers a strong link to keywords of conflict, weapons and discrimination. These factors not only proxy similar firm traits, but are also of similar magnitude, ranging around 0.1. This hints at a strong latent foundation across features, which is by construction from the chosen topics. All ESG topic keywords stem from the idea of negative screening which comprises negative criteria. If there would be a more diverse spectrum of keywords, this likely leads to a less uniform distribution of factor loadings and a less distinct drop of explained variance from one principal component to another. The previous correlation analysis hints at informational uniformity across features, which the PCA confirms. The PCA uncovers that one latent factor accounts for most of the variance, which likely stands for negative associations with a firm. Since the keywords were chosen based on this, it is mostly self-constructed.</p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<h2 id="methodology">Methodology 

</h2><h3 id="benchmark-model">Benchmark model 

</h3><!-- raw HTML omitted -->
<p>Two benchmark classifiers are drawn as comparison to the neural network. The first one is a coin flip and the second one a logistic classifier.</p>
<p>By construction of the outcome variable, its median split serves as naive model, where half of the firms fall into the positive category, even though they perform low on ESG. The naive approach is equivalent to flipping a coin, achieving an accuracy of 50% on average. The second benchmark is a logistic classifier, which is a simple but commonly found classifier.</p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<h3 id="neural-network-with-pytorch-in-sagemaker">Neural network with Pytorch in Sagemaker 

</h3><!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<p>I implemented the neural network with Pytorch on Amazon Sagemaker, with the necessary scripts <code>train.py</code>, <code>model.py</code> and <code>predict.py</code> in the corresponding <code>pytorch_source</code> folder. Furthermore, a Sagemaker Pytorch estimator object was trained using a 80%/20% train-test split. Thereafter, I deployed the model for testing purposes and generated the predictions on the test set. Additionally, all classification metrics were retrieved. All implementation steps for Pytorch were repeated for the Sklearn logistic classifier, excluding the <code>model.py</code> and <code>predict.py</code>scripts. For model comparison, the evaluation metrics of both models were merged. As a last step, Sagemaker&rsquo;s resources were cleaned up, deleting endpoints and data from the S3 bucket.</p>
<p>This is how pytorch instantiated on Sagemaker in detail, specified with 512 hidden dimensions, 40 training epochs and the number of features (145).</p>
<div class="highlight"><pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#a2f;font-weight:bold">from</span> <span style="color:#00f;font-weight:bold">sagemaker.pytorch</span> <span style="color:#a2f;font-weight:bold">import</span> PyTorch

<span style="color:#080;font-style:italic"># select instance</span>
instance <span style="color:#666">=</span>  <span style="color:#b44">&#39;ml.m4.xlarge&#39;</span> 

<span style="color:#080;font-style:italic"># specify output path in S3</span>
output_path <span style="color:#666">=</span> <span style="color:#b44">&#39;s3://{}/{}&#39;</span><span style="color:#666">.</span>format(bucket, prefix)
<span style="color:#a2f;font-weight:bold">print</span>(<span style="color:#b44">&#34;S3 OUTPUT PATH:</span><span style="color:#b62;font-weight:bold">\n</span><span style="color:#b44">{}&#34;</span><span style="color:#666">.</span>format(output_path))

<span style="color:#080;font-style:italic"># instantiate a pytorch estimator</span>
estimator <span style="color:#666">=</span> PyTorch(entry_point<span style="color:#666">=</span><span style="color:#b44">&#39;train.py&#39;</span>,
                   source_dir<span style="color:#666">=</span><span style="color:#b44">&#39;source_pytorch&#39;</span>, 
                   role<span style="color:#666">=</span>role,
                   framework_version<span style="color:#666">=</span><span style="color:#b44">&#39;1.5.0&#39;</span>, <span style="color:#080;font-style:italic">#latest version </span>
                   train_instance_count<span style="color:#666">=</span><span style="color:#666">1</span>, 
                   train_instance_type<span style="color:#666">=</span>instance,
                   output_path<span style="color:#666">=</span>output_path,
                   sagemaker_session<span style="color:#666">=</span>sagemaker_session, 
                   hyperparameters<span style="color:#666">=</span>{
                       <span style="color:#b44">&#39;input_features&#39;</span>: <span style="color:#666">145</span>, 
                       <span style="color:#b44">&#39;hidden_dim&#39;</span>: <span style="color:#666">512</span>,
                       <span style="color:#b44">&#39;output_dim&#39;</span>: <span style="color:#666">1</span>,
                       <span style="color:#b44">&#39;epochs&#39;</span>: <span style="color:#666">40</span>
                   })

<span style="color:#080;font-style:italic"># Train estimator on S3 training data</span>
estimator<span style="color:#666">.</span>fit({<span style="color:#b44">&#39;train&#39;</span>: input_data})
</code></pre></div><p>The logistic classifier was similarly instantiated using a Sklearn estimator object.</p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<h2 id="results">Results 

</h2><h3 id="model-performance">Model Performance 

</h3><!-- raw HTML omitted -->
<p>The final neural network scores 55.7% accuracy, which is slightly better than chance. The logistic classifier scores 7 percentage points lower with an accuracy of 48.7% being worse than a coin flip. This shows that the neural network performs best given the limitations of the data. With more extensive data, having more features and firms, it is expected to perform even better.</p>
<p><p class="markdown-image">
  <img src="/figures/esg_googletrends/fig_performance_metric.png" alt=""  />
</p></p>
<table>
<thead>
<tr>
<th>Model</th>
<th>Precision</th>
<th>Recall</th>
<th>Accuracy</th>
</tr>
</thead>
<tbody>
<tr>
<td>Logistic Classifier</td>
<td>0.487805</td>
<td>0.645161</td>
<td>0.475410</td>
</tr>
<tr>
<td>Neural Network</td>
<td>0.545455</td>
<td>0.774194</td>
<td>0.557377</td>
</tr>
</tbody>
</table>
<p>However, it also consumes disproportionately more computational resources than the simple benchmark. It is up to future work, whether this performance advantage grows or shrinks with more extensive data.</p>
<!-- raw HTML omitted -->
<p>The model builds on Pytorch as a neural network. It consists of three fully connected linear layers with a sigmoid function as the output layer. Feedforward behavior is defined by three  rectified linear units (relu) as hidden layers, two dropout layers to avoid overfitting, and lastly, the sigmoid output layer. The dimensions for hidden layers were chosen to be particularly high to start of with a rather complex model, which could the be pruned to reduce computational expense.</p>
<p>An observation passes through the network as follows. First, the 145 input features pass the first hidden layer with 512 relu, followed by a 20% dropout layer. Second, the signals proceed to the second hidden layer with $512/2=256$ relus, followed again by an equivalent dropout layer, followed by a third hidden layer, which outputs one dimension. Lastly, the signal enters a sigmoid function to predict an outcome.</p>
<!-- raw HTML omitted -->
<p>Lack of data causes the models to score low in accuracy and related metrics. Robustness checks would be sensible if a model is expected to perform as part of an application. However, at such low accuracy, the largest scope for improvement lies in more and high quality data. A matter of concern is high correlation between features, which can be tackled by adding additional features, that correlate negatively among each other, thereby increasing the chances to add predictive power for ESG scores. One example could be financial data, which could be merged to the existing dataset.</p>
<p>Even though, more extensive data likely contributes to the largest accuracy gains, I outline robustness checks below. I leave their implementation to a future version of this project.</p>
<ul>
<li>Model: Change model parameters by increasing the number of hidden layers</li>
<li>Data: Define ESG top performers as firms that fall into the highest 25 percent of ESG scores instead of a median split.</li>
<li>Data: use a continuous outcome such as the  ESG score instead of a binary target and analyze the model&rsquo;s prediction error</li>
<li>Data: drop the time dimension and aggregate across all five years</li>
<li>Data: Check the influence of the decay function dataset with and without time dimension</li>
</ul>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<h2 id="conclusion">Conclusion 

</h2><!-- raw HTML omitted -->
<p>To conclude, I conceptualized, developed and implemented a data analysis project from scratch which mainly deals with search frequency of keywords from Google. Major challenges were faced while collecting the data which compromised the extensiveness of the data and depth of model analysis.</p>
<p>The main question: <em>&ldquo;Can Google trends and machine learning inform investors about a firm&rsquo;s ESG performance?&quot;</em> can be answered with a cautious &lsquo;yes&rsquo;. On the one hand, the data lacked observations from firms and additional keywords. On the other hand, there is great potential to re-use procedures to query Google for follow up and an extension of the project.</p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<hr>
<h2 id="appendix">Appendix 

</h2><h3 id="readme">README 

</h3><p>I conducted this project as part of the Machine Learning Engineer Nanodegree at Udacity in August 2020.</p>
<p><strong>Python libraries</strong></p>
<p>It relies on the following libraries or services</p>
<ul>
<li>Python (3.6)</li>
<li>Pytorch (1.4.0)</li>
<li>Amazon Sagemaker</li>
<li>Pytrends (4.7.3, <a href="https://pypi.org/project/pytrends/">https://pypi.org/project/pytrends/</a>)</li>
<li>Yahooquery (2.2.6, <a href="https://pypi.org/project/yahooquery/">https://pypi.org/project/yahooquery/</a>)</li>
</ul>
<p>The data collection script (<code>1_data_collection.ipynb</code>) takes approximately 17 hours due to Google&rsquo;s rate limit and a timeout function which avoids server errors. It is therefore recommended to load the provided csv data files for replication.</p>
<h3 id="disclaimer">Disclaimer 

</h3><p>This project is meant to be a pilot for a more rigorous approach to ESG related metrics. I use it to make the error-prone interaction with Google trends accessible, establish work flows with Amazon Sagemaker and identify strengths and weaknesses of the self-collected data. The highly customized data is the major strength of this project. Once established processes to collect data can be easily extended and generalize to other problems, where custom data yields an advantage.</p>
<p>But it also has its drawbacks regarding extensiveness, quality and high upfront costs. Collecting web data takes disproportionately more time than a ready-to-load dataset which leaves less time to work on equally important aspects, such as feature engineering, data processing, model selection, tuning and testing.</p>
<p>All in all, the project yields amazing opportunities for follow-up work and great potential to generalize its data gathering process to other fields. This prototype holds valuable insights, especially for fields where data is scarce and customized solutions that rely on Google search data is crucial.</p>
<p>At the end of this report, I outline a road map for future work and possible extensions. These can be used to further carve out a data science portfolio and discover pathways to untapped data.</p>
<h3 id="references">References 

</h3><p>Auer, B. R., &amp; Schuhmacher, F. (2016). Do socially (ir) responsible investments pay? New evidence from international ESG data. The Quarterly Review of Economics and Finance, 59, 51-62.
Bank of America Research (2020). Accessed 28th July 2020,  <a href="https://www.merrilledge.com/article/why-esg-matters#:~:text=Why%20ESG%20matters%20%E2%80%94%20Now%20more%20than%20ever&amp;text=A%20new%20BofA%20Global%20Research,over%20companies%20that%20don't">https://www.merrilledge.com/article/why-esg-matters#:~:text=Why%20ESG%20matters%20%E2%80%94%20Now%20more%20than%20ever&amp;text=A%20new%20BofA%20Global%20Research,over%20companies%20that%20don't</a>.</p>
<p>FTSE Russell (2018). Smart beta: 2018 global survey findings from asset owners. Accessed on 29th July 2020, <a href="https://investmentnews.co.nz/wp-content/uploads/Smartbeta18.pdf">https://investmentnews.co.nz/wp-content/uploads/Smartbeta18.pdf</a>.</p>
<p>Griggs, D., Stafford-Smith, M., Gaffney, O., Rockström, J., Öhman, M. C., Shyamsundar, P., &hellip; &amp; Noble, I. (2013). Sustainable development goals for people and planet. Nature, 495(7441), 305-307.</p>
<p>International Finance Corporation, UN Global Compact, Federal Department of Foreign Affairs Switzerland (2005). Who Cares Wins 2005 Conference Report: Investing for Long-Term Value. Accessed 29th July 2020, <a href="https://www.ifc.org/wps/wcm/connect/topics_ext_content/ifc_external_corporate_site/sustainability-at-ifc/publications/publications_report_whocareswins2005__wci__1319576590784">https://www.ifc.org/wps/wcm/connect/topics_ext_content/ifc_external_corporate_site/sustainability-at-ifc/publications/publications_report_whocareswins2005__wci__1319576590784</a>.</p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<blockquote>
</blockquote>

    </div>

    
        <div class="tags">
            
                <a href="https://philippschmalen.github.io/tags/amazon-sagemaker">Amazon SageMaker</a>
            
                <a href="https://philippschmalen.github.io/tags/pytorch">PyTorch</a>
            
                <a href="https://philippschmalen.github.io/tags/esg">ESG</a>
            
                <a href="https://philippschmalen.github.io/tags/sustainable-investment">Sustainable investment</a>
            
                <a href="https://philippschmalen.github.io/tags/model-deployment">model deployment</a>
            
                <a href="https://philippschmalen.github.io/tags/google-trends">Google trends</a>
            
                <a href="https://philippschmalen.github.io/tags/yahoo-finance">Yahoo finance</a>
            
                <a href="https://philippschmalen.github.io/tags/pytrends">Pytrends</a>
            
                <a href="https://philippschmalen.github.io/tags/yahooquery">yahooquery</a>
            
        </div>
    
    
    

</section>


    </main>
    
    
<footer id="footer">
    
        <div id="social">


    <a class="inline-svg" href="mailto:philippschmalen@gmail.com" target="_blank">
        
        <svg aria-hidden="true" focusable="false" data-prefix="far" data-icon="envelope" class="svg-inline--fa fa-envelope fa-w-16" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M464 64H48C21.49 64 0 85.49 0 112v288c0 26.51 21.49 48 48 48h416c26.51 0 48-21.49 48-48V112c0-26.51-21.49-48-48-48zm0 48v40.805c-22.422 18.259-58.168 46.651-134.587 106.49-16.841 13.247-50.201 45.072-73.413 44.701-23.208.375-56.579-31.459-73.413-44.701C106.18 199.465 70.425 171.067 48 152.805V112h416zM48 400V214.398c22.914 18.251 55.409 43.862 104.938 82.646 21.857 17.205 60.134 55.186 103.062 54.955 42.717.231 80.509-37.199 103.053-54.947 49.528-38.783 82.032-64.401 104.947-82.653V400H48z"></path></svg>
    </a>

    <a class="inline-svg" href="https://github.com/philippschmalen" target="_blank">
        
        <svg aria-hidden="true" focusable="false" data-prefix="fab" data-icon="github" class="svg-inline--fa fa-github fa-w-16" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><path fill="currentColor" d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"></path></svg>
    </a>

    <a class="inline-svg" href="https://www.linkedin.com/in/philippschmalen/" target="_blank">
        
        <svg aria-hidden="true" focusable="false" data-prefix="fab" data-icon="linkedin-in" class="svg-inline--fa fa-linkedin-in fa-w-14" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path fill="currentColor" d="M100.28 448H7.4V148.9h92.88zM53.79 108.1C24.09 108.1 0 83.5 0 53.8a53.79 53.79 0 0 1 107.58 0c0 29.7-24.1 54.3-53.79 54.3zM447.9 448h-92.68V302.4c0-34.7-.7-79.2-48.29-79.2-48.29 0-55.69 37.7-55.69 76.7V448h-92.78V148.9h89.08v40.8h1.3c12.4-23.5 42.69-48.3 87.88-48.3 94 0 111.28 61.9 111.28 142.3V448z"></path></svg>
    </a>

</div>




    

    <p class="copyright">
    
       © Copyright 
       2021 

       Philipp Schmalen
    
    </p>

</footer>

<script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.9.0/katex.min.js" integrity="sha256-34ADEQM6cIZ7chSRA07lN4aD5JM9IQoeIr2VamKDcT0=" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.9.0/contrib/auto-render.min.js" integrity="sha256-HkMrKMLKQk4t1R2ofMAcLz72fWM2sshnx6215U+LgU0=" crossorigin="anonymous"></script>
<script>
  renderMathInElement(document.body,
    {
        delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "$", right: "$", display: false},
        ]
    }
  );

  var inlineMathArray = document.querySelectorAll("script[type='math/tex']");
  for (var i = 0; i < inlineMathArray.length; i++) {
    var inlineMath = inlineMathArray[i];
    var tex = inlineMath.innerText || inlineMath.textContent;
    var replaced = document.createElement("span");
    replaced.innerHTML = katex.renderToString(tex, {displayMode: false});
    inlineMath.parentNode.replaceChild(replaced, inlineMath);
  }

  var displayMathArray = document.querySelectorAll("script[type='math/tex; mode=display']");
  for (var i = 0; i < displayMathArray.length; i++) {
    var displayMath = displayMathArray[i];
    var tex = displayMath.innerHTML;
    var replaced = document.createElement("span");
    replaced.innerHTML = katex.renderToString(tex.replace(/%.*/g, ''), {displayMode: true});
    displayMath.parentNode.replaceChild(replaced, displayMath);
  }
</script>
  </body>
</html>
